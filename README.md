# Веб-корпус с пастами из двача
Точнее из паблика ВК, который их собирает.

Состоит из бэкенда на fastapi, фронта на Vue.js + Vuetify и sqlite-БД. Токенизация, лемматизация, POS-теггинг - это все stanza. Хостится [вот тут](http://91.200.84.6:5002/)

Сделали [Картина Элен](https://github.com/afmigoo) и [Зиновьева Ольга](https://github.com/oilgo) как финальный проект для предмета "Автоматическая обработка данных"

### Кто что делал

| Элен                                              | Ольга                                |
|---------------------------------------------------|--------------------------------------|
| докер                                             | краулинг с помощью VK api            |
| функции токенизации, лемматизации, нахождения POS | заведение БД и функция ее заполнения |
| функция поиска по корпусу                         | функция поиска по корпусу            |
| функция расширения контекста                      | функция расширения контекста         |
| хостинг                                           | фронтенд                             |


### Как запустить у себя

```bash
git clone https://github.com/Affenmilchmann/nlp_final_project_2023.git
```

Создайте файл `./webapp/.env` со следущщим содержимым:
```ini
DATABASE_URL='sqlite:///./instance/corpora.db'
TOKEN_USER=<vkontakte_token_here>
VERSION=5.81
DOMAIN=aveshit
```
(Не обязательно заполнять токен вк. Он нужен для парсинга текстов из вк, которые они уже спаршены в папку [webapp/instance](./webapp/instance/). Так что если вы не будете парсить данные заново он вам не понадобится)

Создайте файл `./frontend/.env` со следущщим содержимым:
```ini
VUE_APP_API_URL=http://localhost:5001
```
(Поменяйте localhost если хостите на другом домене)

```bash
docker compose build
```

```bash
docker compose up
```

#### Схема БД:
![alt text](https://i.imgur.com/XxT43Mu.png)

### Как мы храним и ищем:
1. Мы не считаем нграмы на границах предложений.

    Из текста *"Мама мыла раму. Мыл Петя шнурки."* не будет триграмы *"раму мыл петя"* или *"раму мыл"*.
2. Границы контекста.
    
    - Мы храним границы каждого предложения в БД чтобы моментально узнать/расширить контекст для любой триграмы. (Таблица `sentences`, столбцы `start_index` и `end_index`, они хранят индексы начала и конца предложения в оригинальном тексте.)
    - Также мы храним границы каждого токена в триграме чтобы мочь выделять 1-грамы, биграмы, триграмы в тексте. (Таблица `trigrams`, три столбца `first/second/third_start_index`, три столбца `first/second/third_end_index`. Индексы начала и конца в оригинальном тексте.)
3. Как мы храним и ищем 1-грамы и биграмы:

    Из предложения *"Мама охотно кушает вкусную кашу"* мы кладём в БД следующие триграмы:
    ```
    first   second  third
    -----------------------
    мама    охотно  кушает
    охотно  кушает  вкусную
    кушает  вкусную кашу
    вкусную кашу    NULL
    кашу    NULL    NULL
    ```

    Таким образом мы также можем искать и 1-грамы и биграмы!

    Для поиска 1-грам мы ищем только по `first`-столбцам таблицы `trigrams`, и просто обрезаем контекст по `first_end_index` чтобы получать *"Кушает"* а не *"Кушает вкусную кашу"* при поиске 1-грамы *кушать*.

    Для поиска биграм мы ищем только по `first` и `second`-столбцам и обрезаем контекст по `second_end_index` чтобы получить *"охотно кушает"*, а не *"охотно кушает вкусную"* при поиске биграмы *'охотно' 'кушает'*
    
### Как мы ищем
Для запроса `знать+VERB ADJ NOUN` 

1. Первым делом будут найдены id лемм, словоформ и POS тегов в соответствующих таблиах.

2. Потом с имеющимеся id формируется такой словарь ограничений для последующего SQL запроса

    ```json
    {
        "first_lemma_id": 341,  //id леммы знать,
        "first_pos_id": 3,      //id POS-тега VERB
        "second_pos_id": 2,     //id POS-тега ADJ
        "third_pos_id": 1,      //id POS-тега NOUN
    }
    ```
    И запоминается размер нграмы (чтобы потом обрезать нграму по first/second/third_end_index)

3. Далее будет сформирован SQL подобный запрос:

    ```SQL
    SELECT texts.id AS text_id, texts.full_text, texts.href,
    sentences.id AS sent_id, sentences.start_index, sentences.end_index, 
    trigrams.first_start_index, trigrams.third_end_index
    FROM trigrams
    JOIN sentences  ON sentences.id = trigrams.sentence_id
    JOIN texts      ON texts.id     = sentences.text_id
    WHERE trigrams.first_lemma_id = 341 AND
          trigrams.first_pos_id = 3 AND
          trigrams.second_pos_id = 2 AND
          trigrams.third_pos_id = 1
    ```
    Из которого мы получим элементы выдачи и всё необходимое для каждого элемента: контекст, позиция нграмы в контексте.
